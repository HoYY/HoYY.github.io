I"<p>[환경]<br />
Ubuntu 16.04.3 LTS</p>

<p class="lead">최근 Spark 공부를 하게 되면서 알게 된 내용들을 적어보려고 합니다.<br />
이번 글에서는 Spark를 시작하기 위한 설치 부분과 단일 노드를 사용하는 Standalone 모드에 대해 다뤄보도록 하겠습니다.<br />
Standalone 모드는 다른 클러스터 매니저를 사용하지 않고 Spark 만으로 클러스터를 구성하는 모드를 말한다.<br />
단일 노드로도 구성할 수 있고, 복수의 노드로도 구성할 수 있다.</p>
<p>&lt;!–-break-–&gt;</p>

<h1 id="1-apache-spark란">1. Apache Spark란?</h1>

<p>Apache Spark는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.<br />
Scala, Java, Python 및 R의 고급 API를 지원하는 최적화 엔진을 제공한다.<br />
Spark는 In-Memory 방식으로 Hadoop보다 빠른 속도를 보인다.</p>

<hr />

<h1 id="2-apache-spark-설치">2. Apache Spark 설치</h1>

<p><a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a> 로 들어가서 3. Download Spark 부분을 눌러주면, 다운로드 링크들이 나온다. HTTP아래의 링크를 하나 복사하여 wget을 통해 다운받아준다.</p>

<pre><code># cd ~
# wget http://apache.mirror.cdnetworks.com/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz</code></pre>

<p>다운이 완료되면 tar을 통해 압축을 풀어준다.</p>

<pre><code># tar -xf spark-2.3.1-bin-hadoop2.7.tgz</code></pre>

<p>설치가 완료되면 이제 Spark 사용을 위한 환경설정을 해준다.</p>

<hr />

<h1 id="3-standalone을-위한-환경설정">3. Standalone을 위한 환경설정</h1>

<p>우선 .bashrc 파일에 환경변수를 추가해준다.<br />
SPARK_HOME 환경변수와 Spark의 bin 디렉토리의 경로를 PATH에 추가해준다. MASTER 변수는 편의를 위해 추가해주었다.</p>

<pre><code>.bashrc 파일

# vi .bashrc
export SPARK_HOME=/home/hoyy/spark-2.3.1-bin-hadoop2.7
export PATH=${SPARK_HOME}/bin:${PATH}
export MASTER=spark://spark-master:7077
</code></pre>

<p>다음으로 <strong>${SPAKR_HOME}/conf/</strong> 경로에 있는 <strong>spark-env.sh.temtlate</strong>과 <strong>spark-defaults.conf.template</strong>을 복사한 후 내용을 추가해준다.</p>

<pre><code># cd ${SPARK_HOME}/conf
# cp spark-env.sh.template spark-env.sh
# cp spark-defaults.conf.template spark-defaults.conf
</code></pre>

<p>먼저 <strong>spark-env.sh</strong>에 다음의 내용들을 추가해준다.</p>

<pre><code>spark-env.sh파일

# vi spark-env.sh
export SPARK_MASTER_IP='192.168.80.128'
export SPARK_MASTER_HOST=spark-master
export SPARK_MASTER_WEBUI_PORT=7777
export SPARK_WORKER_INSTANCES=2
export SPARK_EXECUTOR_CORES=1
export SPARK_EXECUTOR_MEMORY=1g
</code></pre>

<p><strong>SPARK_MASTER_IP</strong>는 말 그대로 MASTER 노드의 IP이다. 하나의 노드만 사용할 것이니 현재 노드의 IP를 넣어주자.<br />
<strong>SPAKR_MASTER_HOST</strong>는 MASTER의 호스트이다. 아래에서도 언급하겠지만 spark-env.sh에 적어놓은 호스트를 <strong>/etc/hosts</strong> 파일에도 적어놓아야 한다.<br />
<strong>WORKER_INSTANCES, EXECUTOR_CORES, EXECUTOR_MEMORY</strong>는 각각 worker의 수, executor 하나의 코어 수, executor 하나의 memory이다.<br />
이제 <strong>spark-defaults.conf</strong>의 가장 아래에 있는 <strong>spark.master</strong> 항목과 <strong>spark.serializer</strong> 항목의 ‘#’을 지우고 내용을 수정해준다.</p>

<pre><code>spark-defaults.conf파일

# vi spark-defaults.conf
spark.master        spark://spark-master:7077
spark.serializer    org.apache.spark.serializer.KryoSerializer
</code></pre>

<p>Apache Spark에서는 빅데이터 애플리케이션을 위해 KryoSerializer을 사용하는 것이 좋다고 한다.<br />
마지막으로 <strong>/etc/hosts</strong> 파일에 호스트를 추가해준다.</p>

<pre><code>/etc/hosts파일

# sudo vi /etc/hosts
192.168.80.128    spark-master
</code></pre>

<hr />

<h1 id="4-spark-실행">4. Spark 실행</h1>

<pre><code># cd ${SPARK_HOME}/sbin
# ./start-master.sh    (마스터노드 실행)
# ./start-slave.sh ${MASTER}    (워커노드 실행)
</code></pre>

<p><img src="/uploads/spark_standalone.PNG" alt="Alt text" /></p>

<p>[참고 사이트]<br />
<a href="https://spark.apache.org/docs/latest/spark-standalone.html">https://spark.apache.org/docs/latest/spark-standalone.html</a>
<a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization">http://spark.apache.org/docs/latest/tuning.html#data-serialization</a></p>
:ET